
import os
import re
import requests
import unicodedata
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime

# Configuration des rÃ©pertoires
DATA_DIR = "../data/moteur"
IMAGES_DIR = os.path.join(DATA_DIR, "images")

# CrÃ©er les rÃ©pertoires s'ils n'existent pas
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(IMAGES_DIR, exist_ok=True)

# Configuration de Selenium
options = Options()
options.add_argument("--headless")  # ExÃ©cuter en arriÃ¨re-plan
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
options.add_argument("--disable-blink-features=AutomationControlled")  # Contourner la dÃ©tection des bots

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# URL de base sans numÃ©ro de page
BASE_URL = "https://www.moteur.ma/fr/voiture/achat-voiture-occasion/"

# Initialiser le driver une seule fois
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def extract_id_from_url(url):
    """Extrait l'ID de l'annonce depuis l'URL."""
    match = re.search(r"/detail-annonce/(\d+)/", url)
    return match.group(1) if match else "N/A"

def sanitize_filename(filename):
    """Nettoie un nom de fichier pour qu'il soit valide sur le systÃ¨me d'exploitation."""
    # Remplacer les caractÃ¨res non-alphanumÃ©riques par des underscores
    filename = re.sub(r'[^\w\s-]', '_', filename)
    # Normaliser les caractÃ¨res accentuÃ©s
    filename = unicodedata.normalize('NFKD', filename).encode('ASCII', 'ignore').decode('ASCII')
    # Remplacer les espaces par des underscores
    filename = re.sub(r'\s+', '_', filename)
    return filename

def download_image(url, folder_path, index):
    """TÃ©lÃ©charge une image Ã  partir d'une URL avec des en-tÃªtes amÃ©liorÃ©s."""
    try:
        print(f"TÃ©lÃ©chargement de {url} vers {folder_path}")
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code == 200:
            file_extension = url.split('.')[-1]
            if '?' in file_extension:
                file_extension = file_extension.split('?')[0]
            if not file_extension or len(file_extension) > 5:
                file_extension = "jpg"  # Extension par dÃ©faut si problÃ¨me
            image_path = os.path.join(folder_path, f"image_{index}.{file_extension}")
            with open(image_path, 'wb') as f:
                f.write(response.content)
            print(f"âœ… Image enregistrÃ©e : {image_path}")
            return True
        else:
            print(f"âŒ Erreur HTTP {response.status_code} pour {url}")
        return False
    except Exception as e:
        print(f"âš ï¸ Erreur lors du tÃ©lÃ©chargement de l'image {url}: {e}")
        return False

def scrape_page(page_url):
    """Scrape les annonces d'une page donnÃ©e."""
    driver.get(page_url)
    
    # Attendre que les annonces chargent (timeout de 10 sec)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "row-item"))
        )
    except:
        print(f"Aucune annonce trouvÃ©e sur {page_url}")
        return []
    
    # RÃ©cupÃ©rer les annonces
    car_elements = driver.find_elements(By.CLASS_NAME, "row-item")
    data = []
    
    for car in car_elements:
        try:
            # Titre
            title_element = car.find_element(By.CLASS_NAME, "title_mark_model")
            title = title_element.text.strip() if title_element else "N/A"
            
            # Lien de l'annonce et extraction de l'ID
            try:
                link_element = car.find_element(By.XPATH, ".//h3[@class='title_mark_model']/a")
                link = link_element.get_attribute("href") if link_element else "N/A"
                ad_id = extract_id_from_url(link)  # Extraire l'ID
            except:
                link, ad_id = "N/A", "N/A"
            
            # Prix
            try:
                price_element = car.find_element(By.CLASS_NAME, "PriceListing")
                price = price_element.text.strip()
            except:
                price = "N/A"
            
            # AnnÃ©e, Ville, Carburant (On vÃ©rifie la prÃ©sence)
            meta_elements = car.find_elements(By.TAG_NAME, "li")
            year = meta_elements[1].text.strip() if len(meta_elements) > 1 else "N/A"
            city = meta_elements[2].text.strip() if len(meta_elements) > 2 else "N/A"
            fuel = meta_elements[3].text.strip() if len(meta_elements) > 3 else "N/A"
            
            # Ajouter les donnÃ©es
            data.append({
                "ID": ad_id,
                "Titre": title,
                "Prix": price,
                "AnnÃ©e": year,
                "Ville": city,
                "Carburant": fuel,
                "Lien": link
            })
        
        except Exception as e:
            print(f"Erreur sur une annonce : {e}")
    
    return data

def scrape_multiple_pages(max_pages=1):
    """Scrape plusieurs pages du site en respectant le format de pagination (0, 30, 60, 90)"""
    all_data = []
    
    for page_offset in range(0, max_pages * 30, 30):
        print(f"Scraping page avec offset {page_offset}...")
        page_url = f"{BASE_URL}{page_offset}" if page_offset > 0 else BASE_URL
        all_data.extend(scrape_page(page_url))
        time.sleep(3)  # Pause pour Ã©viter le blocage
    
    return all_data

def scrape_detail_page(url, ad_id, title):
    """Scrape les dÃ©tails d'une annonce spÃ©cifique."""
    try:
        # AccÃ©der Ã  la page de dÃ©tail
        driver.get(url)
        time.sleep(3)  # Attendre le chargement de la page
        
        # CrÃ©er un dossier pour les images de cette annonce
        folder_name = f"{ad_id}_{sanitize_filename(title)}"
        folder_path = os.path.join(IMAGES_DIR, folder_name)
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
            print(f"ğŸ“‚ Dossier crÃ©Ã© : {folder_path}")
        
        # RÃ©cupÃ©rer les dÃ©tails du vÃ©hicule
        details = {}
        
        # 1. Prix
        try:
            price_element = driver.find_element(By.CSS_SELECTOR, "h1 span")
            details["Prix"] = price_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction prix: {e}")
            details["Prix"] = "N/A"
        
        # 2. Informations techniques dans les detail_line
        detail_lines = driver.find_elements(By.CLASS_NAME, "detail_line")
        
        for line in detail_lines:
            try:
                spans = line.find_elements(By.TAG_NAME, "span")
                if len(spans) >= 2:
                    key = spans[0].text.strip()
                    value = spans[1].text.strip()
                    
                    if "KilomÃ©trage" in key:
                        details["KilomÃ©trage"] = value
                    elif "AnnÃ©e" in key:
                        details["AnnÃ©e"] = value
                    elif "Boite de vitesses" in key:
                        details["Transmission"] = value
                    elif "Carburant" in key:
                        details["Type de carburant"] = value
                    elif "Date" in key:
                        details["Date de publication"] = value
                    elif "Puissance" in key:
                        details["Puissance fiscale"] = value
                    elif "Nombre de portes" in key:
                        details["Nombre de portes"] = value
                    elif "PremiÃ¨re main" in key:
                        details["PremiÃ¨re main"] = value
                    elif "VÃ©hicule dÃ©douanÃ©" in key:
                        details["DÃ©douanÃ©"] = value
            except Exception as e:
                print(f"Erreur lors de l'extraction d'une ligne de dÃ©tail: {e}")
        
        # 3. Description
        try:
            description_element = driver.find_element(By.CSS_SELECTOR, "div.options div.col-md-12")
            details["Description"] = description_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction description: {e}")
            details["Description"] = "N/A"
        
        # 4. Nom du vendeur
        try:
            seller_element = driver.find_element(By.CSS_SELECTOR, "a[href*='stock-professionnel']")
            details["CrÃ©ateur"] = seller_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction vendeur: {e}")
            details["CrÃ©ateur"] = "N/A"
        
        # 5. Ville
        try:
            city_element = driver.find_element(By.CSS_SELECTOR, "a[href*='ville']")
            details["Ville"] = city_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction ville: {e}")
            details["Ville"] = "N/A"
        
        # 6. Images - MÃ©thode amÃ©liorÃ©e de tÃ©lÃ©chargement
        image_count = 0
        try:
            # Trouver les Ã©lÃ©ments d'image
            image_elements = driver.find_elements(By.CSS_SELECTOR, "img[data-u='image']")
            
            if not image_elements:
                print("âš ï¸ Aucune image trouvÃ©e sur la page. Essai d'une sÃ©lection alternative...")
                # Essayer une autre mÃ©thode de sÃ©lection
                image_elements = driver.find_elements(By.CSS_SELECTOR, ".swiper-slide img")
                
            print(f"TrouvÃ© {len(image_elements)} images potentielles")
            
            for index, img in enumerate(image_elements):
                img_url = img.get_attribute("src")
                if img_url and "http" in img_url:
                    success = download_image(img_url, folder_path, index + 1)
                    if success:
                        image_count += 1
                else:
                    print(f"URL d'image invalide: {img_url}")
            
            # Si toujours pas d'images, essayer de chercher dans le code source
            if image_count == 0:
                print("Recherche d'images dans le code source...")
                page_source = driver.page_source
                img_urls = re.findall(r'src=[\'"]([^\'"]*\.(?:jpg|jpeg|png|gif)(?:\?[^\'"]*)?)[\'"]', page_source)
                for index, img_url in enumerate(set(img_urls)):
                    if "http" in img_url and "thumb" not in img_url.lower():
                        success = download_image(img_url, folder_path, index + 1)
                        if success:
                            image_count += 1
            
            print(f"ğŸ“¸ {image_count} images tÃ©lÃ©chargÃ©es pour {title}")
        except Exception as e:
            print(f"Erreur lors de l'extraction des images: {e}")
        
        # Ajouter le nombre d'images tÃ©lÃ©chargÃ©es
        details["Nombre d'images"] = str(image_count)
        
        # Ajouter l'ID, le titre, l'URL et le dossier d'images
        details["ID"] = ad_id
        details["Titre"] = title
        details["URL de l'annonce"] = url
        details["Dossier d'images"] = folder_name
        
        return details
        
    except Exception as e:
        print(f"âŒ Erreur lors du scraping de la page {url}: {e}")
        return {
            "ID": ad_id,
            "Titre": title,
            "URL de l'annonce": url,
            "Erreur": str(e)
        }

def main():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(DATA_DIR, f"moteur_ma_data_{timestamp}.csv")
    
    try:
        # Ã‰tape 1: RÃ©cupÃ©rer les annonces des pages de liste
        print("ğŸ” DÃ©marrage du scraping des pages de liste...")
        car_listings = scrape_multiple_pages(max_pages=1)  # 4 pages (0, 30, 60, 90)
        print(f"âœ… Scraping des listes terminÃ© ! {len(car_listings)} annonces trouvÃ©es.")
        
        # Sauvegarde intermÃ©diaire (optionnelle)
        listings_df = pd.DataFrame(car_listings)
        temp_csv_path = os.path.join(DATA_DIR, "temp_listings.csv")
        listings_df.to_csv(temp_csv_path, index=False, encoding="utf-8-sig")
        print(f"ğŸ’¾ Sauvegarde intermÃ©diaire des listings dans {temp_csv_path}")
        
        # Ã‰tape 2: Extraire les dÃ©tails pour chaque annonce
        print("\nğŸ” DÃ©marrage de l'extraction des dÃ©tails pour chaque annonce...")
        detailed_data = []
        
        for index, listing in enumerate(car_listings):
            try:
                ad_id = listing["ID"]
                title = listing["Titre"]
                link = listing["Lien"]
                
                print(f"[{index+1}/{len(car_listings)}] Scraping de l'annonce: {title}")
                
                if link and link != "N/A" and "http" in link:
                    # Extraire les dÃ©tails de la page
                    details = scrape_detail_page(link, ad_id, title)
                    detailed_data.append(details)
                    
                    # Pause pour Ã©viter le blocage
                    time.sleep(2 + (index % 3))  # Pause variable entre 2 et 4 secondes
                else:
                    print(f"âŒ Lien invalide pour l'annonce {ad_id}: {link}")
                    detailed_data.append({
                        "ID": ad_id,
                        "Titre": title,
                        "URL de l'annonce": link,
                        "Erreur": "Lien invalide"
                    })
            except Exception as e:
                print(f"âŒ Erreur lors du traitement de l'annonce {index}: {e}")
        
        # Ã‰tape 3: Convertir en DataFrame et enregistrer
        print("\nğŸ’¾ PrÃ©paration et sauvegarde des donnÃ©es complÃ¨tes...")
        result_df = pd.DataFrame(detailed_data)
        
        # RÃ©organiser les colonnes
        columns_order = [
            "ID", "Titre", "Prix", "Date de publication", "AnnÃ©e", 
            "Type de carburant", "Transmission", "KilomÃ©trage", 
            "Puissance fiscale", "Nombre de portes", "PremiÃ¨re main", 
            "DÃ©douanÃ©", "Description", "Ville", "CrÃ©ateur", 
            "URL de l'annonce", "Dossier d'images", "Nombre d'images"
        ]
        
        # Filtrer pour inclure seulement les colonnes prÃ©sentes
        actual_columns = [col for col in columns_order if col in result_df.columns]
        result_df = result_df[actual_columns]
        
        # Enregistrer les donnÃ©es
        result_df.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"âœ… Scraping complet terminÃ© ! {len(result_df)} annonces traitÃ©es.")
        print(f"ğŸ“Š DonnÃ©es enregistrÃ©es dans {output_file}")
        
        # Supprimer le fichier temporaire
        if os.path.exists(temp_csv_path):
            os.remove(temp_csv_path)
        
        # Afficher des statistiques
        successful_images = sum(int(row.get("Nombre d'images", 0)) for _, row in result_df.iterrows())
        print(f"ğŸ“Š Statistiques:")
        print(f"  - Annonces traitÃ©es: {len(result_df)}")
        print(f"  - Images tÃ©lÃ©chargÃ©es: {successful_images}")
        print(f"  - Moyenne d'images par annonce: {successful_images/len(result_df) if len(result_df) > 0 else 0:.1f}")
        
    except Exception as e:
        print(f"âŒ Erreur globale: {e}")
    finally:
        # Fermer le navigateur
        driver.quit()
        print("ğŸ Programme terminÃ©.")

if __name__ == "__main__":
    main()