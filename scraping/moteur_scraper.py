
import os
import re
import requests
import unicodedata
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime

# Configuration des r√©pertoires
DATA_DIR = "../data/moteur"
IMAGES_DIR = os.path.join(DATA_DIR, "images")

# Cr√©er les r√©pertoires s'ils n'existent pas
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(IMAGES_DIR, exist_ok=True)

# Configuration de Selenium
options = Options()
options.add_argument("--headless")  # Ex√©cuter en arri√®re-plan
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
options.add_argument("--disable-blink-features=AutomationControlled")  # Contourner la d√©tection des bots

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# URL de base sans num√©ro de page
BASE_URL = "https://www.moteur.ma/fr/voiture/achat-voiture-occasion/"

# Initialiser le driver une seule fois
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def extract_id_from_url(url):
    """Extrait l'ID de l'annonce depuis l'URL."""
    match = re.search(r"/detail-annonce/(\d+)/", url)
    return match.group(1) if match else "N/A"

def sanitize_filename(filename):
    """Nettoie un nom de fichier pour qu'il soit valide sur le syst√®me d'exploitation."""
    # Remplacer les caract√®res non-alphanum√©riques par des underscores
    filename = re.sub(r'[^\w\s-]', '_', filename)
    # Normaliser les caract√®res accentu√©s
    filename = unicodedata.normalize('NFKD', filename).encode('ASCII', 'ignore').decode('ASCII')
    # Remplacer les espaces par des underscores
    filename = re.sub(r'\s+', '_', filename)
    return filename

def download_image(url, folder_path, index):
    """T√©l√©charge une image √† partir d'une URL avec des en-t√™tes am√©lior√©s."""
    try:
        print(f"T√©l√©chargement de {url} vers {folder_path}")
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code == 200:
            file_extension = url.split('.')[-1]
            if '?' in file_extension:
                file_extension = file_extension.split('?')[0]
            if not file_extension or len(file_extension) > 5:
                file_extension = "jpg"  # Extension par d√©faut si probl√®me
            image_path = os.path.join(folder_path, f"image_{index}.{file_extension}")
            with open(image_path, 'wb') as f:
                f.write(response.content)
            print(f"‚úÖ Image enregistr√©e : {image_path}")
            return True
        else:
            print(f"‚ùå Erreur HTTP {response.status_code} pour {url}")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors du t√©l√©chargement de l'image {url}: {e}")
        return False

def scrape_page(page_url):
    """Scrape les annonces d'une page donn√©e."""
    driver.get(page_url)
    
    # Attendre que les annonces chargent (timeout de 10 sec)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "row-item"))
        )
    except:
        print(f"Aucune annonce trouv√©e sur {page_url}")
        return []
    
    # R√©cup√©rer les annonces
    car_elements = driver.find_elements(By.CLASS_NAME, "row-item")
    data = []
    
    for car in car_elements:
        try:
            # Titre
            title_element = car.find_element(By.CLASS_NAME, "title_mark_model")
            title = title_element.text.strip() if title_element else "N/A"
            
            # Lien de l'annonce et extraction de l'ID
            try:
                link_element = car.find_element(By.XPATH, ".//h3[@class='title_mark_model']/a")
                link = link_element.get_attribute("href") if link_element else "N/A"
                ad_id = extract_id_from_url(link)  # Extraire l'ID
            except:
                link, ad_id = "N/A", "N/A"
            
            # Prix
            try:
                price_element = car.find_element(By.CLASS_NAME, "PriceListing")
                price = price_element.text.strip()
            except:
                price = "N/A"
            
            # Ann√©e, Ville, Carburant (On v√©rifie la pr√©sence)
            meta_elements = car.find_elements(By.TAG_NAME, "li")
            year = meta_elements[1].text.strip() if len(meta_elements) > 1 else "N/A"
            city = meta_elements[2].text.strip() if len(meta_elements) > 2 else "N/A"
            fuel = meta_elements[3].text.strip() if len(meta_elements) > 3 else "N/A"
            
            # Ajouter les donn√©es
            data.append({
                "ID": ad_id,
                "Titre": title,
                "Prix": price,
                "Ann√©e": year,
                "Ville": city,
                "Carburant": fuel,
                "Lien": link
            })
        
        except Exception as e:
            print(f"Erreur sur une annonce : {e}")
    
    return data

def scrape_multiple_pages(max_pages=1):
    """Scrape plusieurs pages du site en respectant le format de pagination (0, 30, 60, 90)"""
    all_data = []
    
    for page_offset in range(0, max_pages * 30, 30):
        print(f"Scraping page avec offset {page_offset}...")
        page_url = f"{BASE_URL}{page_offset}" if page_offset > 0 else BASE_URL
        all_data.extend(scrape_page(page_url))
        time.sleep(3)  # Pause pour √©viter le blocage
    
    return all_data

def scrape_detail_page(url, ad_id, title):
    """Scrape les d√©tails d'une annonce sp√©cifique."""
    try:
        # Acc√©der √† la page de d√©tail
        driver.get(url)
        time.sleep(3)  # Attendre le chargement de la page
        
        # Cr√©er un dossier pour les images de cette annonce
        folder_name = f"{ad_id}_{sanitize_filename(title)}"
        folder_path = os.path.join(IMAGES_DIR, folder_name)
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
            print(f"üìÇ Dossier cr√©√© : {folder_path}")
        
        # R√©cup√©rer les d√©tails du v√©hicule
        details = {}
        
        # 1. Prix
        try:
            price_element = driver.find_element(By.CSS_SELECTOR, "h1 span")
            details["Prix"] = price_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction prix: {e}")
            details["Prix"] = "N/A"
        
        # 2. Informations techniques dans les detail_line
        detail_lines = driver.find_elements(By.CLASS_NAME, "detail_line")
        
        for line in detail_lines:
            try:
                spans = line.find_elements(By.TAG_NAME, "span")
                if len(spans) >= 2:
                    key = spans[0].text.strip()
                    value = spans[1].text.strip()
                    
                    if "Kilom√©trage" in key:
                        details["Kilom√©trage"] = value
                    elif "Ann√©e" in key:
                        details["Ann√©e"] = value
                    elif "Boite de vitesses" in key:
                        details["Transmission"] = value
                    elif "Carburant" in key:
                        details["Type de carburant"] = value
                    elif "Date" in key:
                        details["Date de publication"] = value
                    elif "Puissance" in key:
                        details["Puissance fiscale"] = value
                    elif "Nombre de portes" in key:
                        details["Nombre de portes"] = value
                    elif "Premi√®re main" in key:
                        details["Premi√®re main"] = value
                    elif "V√©hicule d√©douan√©" in key:
                        details["D√©douan√©"] = value
            except Exception as e:
                print(f"Erreur lors de l'extraction d'une ligne de d√©tail: {e}")
        
        # 3. Description
        try:
            description_element = driver.find_element(By.CSS_SELECTOR, "div.options div.col-md-12")
            details["Description"] = description_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction description: {e}")
            details["Description"] = "N/A"
        
        # 4. Nom du vendeur
        try:
            seller_element = driver.find_element(By.CSS_SELECTOR, "a[href*='stock-professionnel']")
            details["Cr√©ateur"] = seller_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction vendeur: {e}")
            details["Cr√©ateur"] = "N/A"
        
        # 5. Ville
        try:
            city_element = driver.find_element(By.CSS_SELECTOR, "a[href*='ville']")
            details["Ville"] = city_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction ville: {e}")
            details["Ville"] = "N/A"
        
        # 6. Images - M√©thode am√©lior√©e de t√©l√©chargement
        image_count = 0
        try:
            # Trouver les √©l√©ments d'image
            image_elements = driver.find_elements(By.CSS_SELECTOR, "img[data-u='image']")
            
            if not image_elements:
                print("‚ö†Ô∏è Aucune image trouv√©e sur la page. Essai d'une s√©lection alternative...")
                # Essayer une autre m√©thode de s√©lection
                image_elements = driver.find_elements(By.CSS_SELECTOR, ".swiper-slide img")
                
            print(f"Trouv√© {len(image_elements)} images potentielles")
            
            for index, img in enumerate(image_elements):
                img_url = img.get_attribute("src")
                if img_url and "http" in img_url:
                    success = download_image(img_url, folder_path, index + 1)
                    if success:
                        image_count += 1
                else:
                    print(f"URL d'image invalide: {img_url}")
            
            # Si toujours pas d'images, essayer de chercher dans le code source
            if image_count == 0:
                print("Recherche d'images dans le code source...")
                page_source = driver.page_source
                img_urls = re.findall(r'src=[\'"]([^\'"]*\.(?:jpg|jpeg|png|gif)(?:\?[^\'"]*)?)[\'"]', page_source)
                for index, img_url in enumerate(set(img_urls)):
                    if "http" in img_url and "thumb" not in img_url.lower():
                        success = download_image(img_url, folder_path, index + 1)
                        if success:
                            image_count += 1
            
            print(f"üì∏ {image_count} images t√©l√©charg√©es pour {title}")
        except Exception as e:
            print(f"Erreur lors de l'extraction des images: {e}")
        
        # Ajouter le nombre d'images t√©l√©charg√©es
        details["Nombre d'images"] = str(image_count)
        
        # Ajouter l'ID, le titre, l'URL et le dossier d'images
        details["ID"] = ad_id
        details["Titre"] = title
        details["URL de l'annonce"] = url
        details["Dossier d'images"] = folder_name
        
        return details
        
    except Exception as e:
        print(f"‚ùå Erreur lors du scraping de la page {url}: {e}")
        return {
            "ID": ad_id,
            "Titre": title,
            "URL de l'annonce": url,
            "Erreur": str(e)
        }

def main():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(DATA_DIR, f"moteur_ma_data_{timestamp}.csv")
    
    try:
        # √âtape 1: R√©cup√©rer les annonces des pages de liste
        print("üîç D√©marrage du scraping des pages de liste...")
        car_listings = scrape_multiple_pages(max_pages=1)  # 4 pages (0, 30, 60, 90)
        print(f"‚úÖ Scraping des listes termin√© ! {len(car_listings)} annonces trouv√©es.")
        
        # Sauvegarde interm√©diaire (optionnelle)
        listings_df = pd.DataFrame(car_listings)
        temp_csv_path = os.path.join(DATA_DIR, "temp_listings.csv")
        listings_df.to_csv(temp_csv_path, index=False, encoding="utf-8-sig")
        print(f"üíæ Sauvegarde interm√©diaire des listings dans {temp_csv_path}")
        
        # √âtape 2: Extraire les d√©tails pour chaque annonce
        print("\nüîé D√©marrage de l'extraction des d√©tails pour chaque annonce...")
        detailed_data = []
        
        for index, listing in enumerate(car_listings):
            try:
                ad_id = listing["ID"]
                title = listing["Titre"]
                link = listing["Lien"]
                
                print(f"[{index+1}/{len(car_listings)}] Scraping de l'annonce: {title}")
                
                if link and link != "N/A" and "http" in link:
                    # Extraire les d√©tails de la page
                    details = scrape_detail_page(link, ad_id, title)
                    detailed_data.append(details)
                    
                    # Pause pour √©viter le blocage
                    time.sleep(2 + (index % 3))  # Pause variable entre 2 et 4 secondes
                else:
                    print(f"‚ùå Lien invalide pour l'annonce {ad_id}: {link}")
                    detailed_data.append({
                        "ID": ad_id,
                        "Titre": title,
                        "URL de l'annonce": link,
                        "Erreur": "Lien invalide"
                    })
            except Exception as e:
                print(f"‚ùå Erreur lors du traitement de l'annonce {index}: {e}")
        
        # √âtape 3: Convertir en DataFrame et enregistrer
        print("\nüíæ Pr√©paration et sauvegarde des donn√©es compl√®tes...")
        result_df = pd.DataFrame(detailed_data)
        
        # R√©organiser les colonnes
        columns_order = [
            "ID", "Titre", "Prix", "Date de publication", "Ann√©e", 
            "Type de carburant", "Transmission", "Kilom√©trage", 
            "Puissance fiscale", "Nombre de portes", "Premi√®re main", 
            "D√©douan√©", "Description", "Ville", "Cr√©ateur", 
            "URL de l'annonce", "Dossier d'images", "Nombre d'images"
        ]
        
        # Filtrer pour inclure seulement les colonnes pr√©sentes
        actual_columns = [col for col in columns_order if col in result_df.columns]
        result_df = result_df[actual_columns]
        
        # Enregistrer les donn√©es
        result_df.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"‚úÖ Scraping complet termin√© ! {len(result_df)} annonces trait√©es.")
        print(f"üìä Donn√©es enregistr√©es dans {output_file}")
        
        # Supprimer le fichier temporaire
        if os.path.exists(temp_csv_path):
            os.remove(temp_csv_path)
        
        # Afficher des statistiques
        successful_images = sum(int(row.get("Nombre d'images", 0)) for _, row in result_df.iterrows())
        print(f"üìä Statistiques:")
        print(f"  - Annonces trait√©es: {len(result_df)}")
        print(f"  - Images t√©l√©charg√©es: {successful_images}")
        print(f"  - Moyenne d'images par annonce: {successful_images/len(result_df) if len(result_df) > 0 else 0:.1f}")
        
    except Exception as e:
        print(f"‚ùå Erreur globale: {e}")
    finally:
        # Fermer le navigateur
        driver.quit()
        print("üèÅ Programme termin√©.")

if __name__ == "__main__":
    main()